<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->

<configuration>
  <!-- logstash.conf -->
  <property>
    <name>content</name>
    <description>This is logstash configuration file to define input, filter and output.</description>
    <value>
#pipeline of hdfs namenode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-namenode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.namenode.log"
    start_position => beginning
    type => "hdfs.namenode"
  }
}

#pipeline of hdfs secondarynamenode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-secondarynamenode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.secondarynamenode.log"
    start_position => beginning
    type => "hdfs.secondarynamenode"
  }
}

#pipeline of hdfs journalnode logs
input {
  file {
    path => "/{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-journalnode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.journalnode.log"
    start_position => beginning
    type => "hdfs.journalnode"
  }
}

#pipeline of hdfs datanode logs
input {
  file {
    path => "{{hdfs_log_dir_prefix}}/{{hdfs_user}}/*-datanode-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hdfs.datanode.log"
    start_position => beginning
    type => "hdfs.datanode"
  }
}

filter {
  if [type] == "hdfs.namenode" or [type] == "hdfs.secondarynamenode" or [type] == "hdfs.journalnode" or [type] == "hdfs.datanode"{
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
    mutate {
      replace => ["timestamp","%{timestamp}+08:00"]
    }
  }
}

output {
  if [type] == "hdfs.namenode" or [type] == "hdfs.secondarynamenode" or [type] == "hdfs.journalnode" or [type] == "hdfs.datanode"{
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hdfs-log"
      template => "{{logstash_log_dir}}/template-service-log.json"
      template_name => "logstash-service-log"
      template_overwrite => true
    }
  }
}

#pipeline of yarn nodemanager logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-nodemanager-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.nodemanager.log"
    start_position => beginning
    type => "yarn.nodemanager"
  }
}

#pipeline of yarn resourcemanager logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-resourcemanager-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.resourcemanager.log"
    start_position => beginning
    type => "yarn.resourcemanager"
  }
}

#pipeline of yarn timelineserver logs
input {
  file {
    path => "{{yarn_log_dir_prefix}}/{{yarn_user}}/*-timelineserver-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/yarn.timelineserver.log"
    start_position => beginning
    type => "yarn.timelineserver"
  }
}

filter {
  if [type] == "yarn.nodemanager" or [type] == "yarn.resourcemanager" or [type] == "yarn.timelineserver"{
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
    mutate {
      replace => ["timestamp","%{timestamp}+08:00"]
    }
  }
}

output {
  if [type] == "yarn.nodemanager" or [type] == "yarn.resourcemanager" or [type] == "yarn.timelineserver"{
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-yarn-log"
      template => "{{logstash_log_dir}}/template-service-log.json"
      template_name => "logstash-service-log"
      template_overwrite => true
    }
  }
}

#pipeline of hbase master logs
input {
  file {
    path => "{{hbase_log_dir}}/*-master-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hbase.master.log"
    start_position => beginning
    type => "hbase.master"
  }
}

#pipeline of hbase regionserver logs
input {
  file {
    path => "{{hbase_log_dir}}/*-regionserver-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hbase.regionserver.log"
    start_position => beginning
    type => "hbase.regionserver"
  }
}

#pipeline of hbase phoenix server logs
input {
  file {
    path => "{{hbase_log_dir}}/phoenix-*.log"
    sincedb_path => "{{logstash_sincedb_path}}/hbase.phoenix-server.log"
    start_position => beginning
    type => "hbase.phoenix"
  }
}

filter {
  if [type] == "hbase.master" or [type] == "hbase.regionserver" or [type] == "hbase.phoenix"{
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
    mutate {
      replace => ["timestamp","%{timestamp}+08:00"]
    }
  }
}

output {
  if [type] == "hbase.master" or [type] == "hbase.regionserver" or [type] == "hbase.phoenix"{
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hbase-log"
      template => "{{logstash_log_dir}}/template-service-log.json"
      template_name => "logstash-service-log"
      template_overwrite => true
    }
  }
}

#pipeline of zookeeper server logs
input {
  file {
    path => "{{zk_log_dir}}/*-server-*.out"
    sincedb_path => "{{logstash_sincedb_path}}/zookeeper.server.log"
    start_position => beginning
    type => "zookeeper.server"
  }
}

filter {
  if [type] == "zookeeper.server" {
    grok {
      match => {   
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+-\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
    mutate {
      replace => ["timestamp","%{timestamp}+08:00"]
    }
  }
}

output {
  if [type] == "zookeeper.server" {
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-zookeeper-log"
      template => "{{logstash_log_dir}}/template-service-log.json"
      template_name => "logstash-service-log"
      template_overwrite => true
    }
  }
}

#pipeline of hive metastore logs
input {
  file {
    path => "{{hive_log_dir}}/hivemetastore.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.metastore.log"
    start_position => beginning
    type => "hive.metastore"
  }
}


#pipeline of hive server2 logs
input {
  file {
    path => "{{hive_log_dir}}/hiveserver2.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.server2.log"
    start_position => beginning
    type => "hive.server2"
  }
}

#pipeline of hive WebHCat Server logs
input {
  file {
    path => "{{webhcat_log_dir}}/webhcat.log"
    sincedb_path => "{{logstash_sincedb_path}}/hive.webhcat.log"
    start_position => beginning
    type => "hive.webhcat"
  }
}

filter {
  if [type] == "hive.metastore" or [type] == "hive.server2" or [type] == "hive.webhcat"{
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:loglevel}\s+%{GREEDYDATA:content}"
      }
    }
    if [loglevel] != "ERROR" and [loglevel] != "WARN" and [loglevel] != "FATAL" {
      drop {}
    }
    mutate {
      replace => ["timestamp","%{timestamp}+08:00"]
    }
  }
}

output {
  if [type] == "hive.metastore" or [type] == "hive.server2" or [type] == "hive.webhcat"{
    elasticsearch {
      hosts => [{% for node in elastic_data_hosts %}"{{node}}:{{elastic_port}}"{% if not loop.last %},{% endif %}{% endfor %}]
      index => "logstash-hive-log"
      template => "{{logstash_log_dir}}/template-service-log.json"
      template_name => "logstash-service-log"
      template_overwrite => true
    }
  }
}
    </value>
    <value-attributes>
      <show-property-name>false</show-property-name>
    </value-attributes>
  </property>
</configuration>
